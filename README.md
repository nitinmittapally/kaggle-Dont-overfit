# Don't Overfit

This another one of the legendary kaggle competition that helps in learning core skill in building ML models- Feature Selection. 
The number of features in this dataset are more than the number of rows, so guess what the OLS method doesn't work here but, yeah gradient descent still works. 
This project teaches Garbage In -> Garbage Out. 

Built DTs, Random forest to understand most important features. 
Built DTs, Random forest, logit models on the selected features. 

