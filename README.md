# Don't Overfit

This another one of the legendary kaggle competition that helps in learning core skill in building ML models- Feature Selection. 
The aim is to predict binary variable for 20K observations based on 200 columns and traning set has around 200 observations. 
So any model we try fit directly overfits the data. Has really good training performance but really sucks on the test data. 
This project teaches Garbage In -> Garbage Out. 

Built DTs, Random forest to understand most important features. 
Built DTs, Random forest, logit, KNN models on the selected feature.
Tuned Hyper-parameters for the models, tried shrinkage methods Lasso and Ridge. 
Built DNNs to automatically find the patterns
used forward feature selection method to select top important features. 
Performed Multivarite analysis to reduce the number of features


